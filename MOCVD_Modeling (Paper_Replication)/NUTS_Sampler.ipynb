{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331c8db-5493-4615-94cc-8eaec9998574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, Lambda,Conv1D,Conv2DTranspose, LeakyReLU,Activation,Flatten,Reshape\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import load_model\n",
    "import emcee\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6810e4d3-cad2-4174-a404-34f03702f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "JV_raw = np.loadtxt(\"/mnt/c/Users/pssrg/Downloads/GaAs_sim_nJV.txt\")\n",
    "par = np.loadtxt(\"/mnt/c/Users/pssrg/Downloads/GaAs_sim_label.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d65422-f2b7-4c7f-a24e-0d1a934f3794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1DTranspose(input_tensor, filters, kernel_size, strides ):\n",
    "    \n",
    "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(input_tensor)\n",
    "    x = Conv2DTranspose(filters=filters, kernel_size=(kernel_size, 1), strides=(strides, 1),padding='SAME')(x)\n",
    "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
    "    return x\n",
    "\n",
    "#Covert labels from log10 form to log\n",
    "        \n",
    "def log10_ln(x):\n",
    "    return np.log(np.power(10,x))\n",
    "\n",
    "par = log10_ln(par)\n",
    "\n",
    "\n",
    "#Data normalization for the whole JV dataset\n",
    "\n",
    "def min_max(x):\n",
    "    min = np.min(x)\n",
    "    max = np.max(x)\n",
    "    return (x-min)/(max-min),max,min\n",
    "\n",
    "#Normalize raw JV data\n",
    "\n",
    "JV_norm,JV_max,JV_min = min_max(JV_raw)\n",
    "\n",
    "#Normalize JV descriptors column-wise\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "par_n = scaler.fit_transform(par)   \n",
    "\n",
    "#create training and testing datset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(JV_norm,par_n, test_size=0.2)\n",
    "\n",
    "#add in Gaussian noise to train the denoising Autoencoder\n",
    "\n",
    "X_train_nos = X_train+0.002 * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape) \n",
    "\n",
    "X_test_nos = X_test+0.002 * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e4dda-1342-4079-90d5-76d25240ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "\n",
    "def build_regression_model(label_dim=5, max_filter=256, map_size = 25, kernel=[7, 5, 3], strides=[5, 2, 2]):\n",
    "    z_in = tf.keras.Input(shape=(label_dim,))  # input: material descriptors (5,)\n",
    "    \n",
    "    # Dense expansion\n",
    "    z = layers.Dense(100, activation='relu')(z_in)\n",
    "    z = layers.Dense(max_filter * map_size, activation='relu')(z)\n",
    "    z = layers.Reshape((map_size, 1, max_filter))(z)\n",
    "    \n",
    "    # Deconvolution blocks\n",
    "    z = layers.Conv2DTranspose(max_filter // 2, (kernel[2], 1), strides=(strides[2], 1), padding='same')(z)\n",
    "    z = layers.Activation('relu')(z)\n",
    "    \n",
    "    z = layers.Conv2DTranspose(max_filter // 4, (kernel[1], 1), strides=(strides[1], 1), padding='same')(z)\n",
    "    z = layers.Activation('relu')(z)\n",
    "    \n",
    "    z = layers.Conv2DTranspose(1, (kernel[0], 1), strides=(strides[0], 1), padding='same')(z)\n",
    "    z = layers.Activation('sigmoid')(z)\n",
    "\n",
    "    # Remove singleton dimensions (squeeze)\n",
    "    z = layers.Lambda(lambda x: tf.squeeze(x, axis=2))(z)  # remove width = 1\n",
    "    z = layers.Lambda(lambda x: tf.squeeze(x, axis=2))(z)  # remove channel = 1\n",
    "\n",
    "    model = tf.keras.Model(inputs=z_in, outputs=z)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f423c4-c5d2-4044-b7d8-2ef9c326a0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = build_regression_model()\n",
    "reg_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "reg_model.fit(par_n, JV_norm, batch_size=128, epochs=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7bed87-68ba-4541-ba81-2314efd31f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = reg_model(y_train, training = False)\n",
    "y_hat_test = reg_model(y_test, training = False)\n",
    "\n",
    "\n",
    "y_hat_train_np = y_hat_train.numpy()\n",
    "y_hat_test_np = y_hat_test.numpy()\n",
    "X_train_np = X_train.numpy() if isinstance(X_train, tf.Tensor) else X_train\n",
    "X_test_np = X_test.numpy() if isinstance(X_test, tf.Tensor) else X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbe59b-f231-4978-93d9-7c5ff2817425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "# ------------- Inputs -------------\n",
    "T_vals = np.array([530, 580, 630, 650, 680], dtype=np.float32)\n",
    "x_vals = -1000.0 / T_vals\n",
    "JV_exp = tf.convert_to_tensor(np.loadtxt('/mnt/c/Users/pssrg/Downloads/GaAs_exp_nJV.txt'), dtype=tf.float32)\n",
    "\n",
    "# ------------- Log-Probability Function -------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def log_prob(*theta):\n",
    "    x = tf.convert_to_tensor(x_vals, dtype=tf.float32)\n",
    "    inv_x = -1.0 / x\n",
    "    log_input = tf.clip_by_value(inv_x, 1e-6, 1e6)\n",
    "\n",
    "    try:\n",
    "        if tf.reduce_any([tf.math.is_nan(t) for t in theta]):\n",
    "            tf.print(\"ðŸ’€ NaN detected in Î¸ input â†’ skipping this point\")\n",
    "            return tf.constant(-1e6, dtype=tf.float32)\n",
    "\n",
    "        a1, b1, c1, a2, b2, c2, a3, b3, c3, a4, b4, c4, a5, b5, c5 = theta\n",
    "        params = tf.stack(theta)\n",
    "        bounds_low = tf.constant([-5.0, -10.0, -10.0] * 5, dtype=tf.float32)\n",
    "        bounds_high = tf.constant([5.0, 10.0, 10.0] * 5, dtype=tf.float32)\n",
    "\n",
    "        if tf.reduce_any(tf.logical_or(params < bounds_low, params > bounds_high)):\n",
    "            return tf.constant(-1e6, dtype=tf.float32)\n",
    "\n",
    "        def descriptor_fn(a, b, c):\n",
    "            return a * tf.math.log(log_input) + b * x + c\n",
    "\n",
    "        descriptor_scaled = 10 * tf.stack([\n",
    "            descriptor_fn(a1, b1, c1),\n",
    "            descriptor_fn(a2, b2, c2),\n",
    "            descriptor_fn(a3, b3, c3),\n",
    "            descriptor_fn(a4, b4, c4),\n",
    "            descriptor_fn(a5, b5, c5),\n",
    "        ], axis=-1)\n",
    "\n",
    "        jv_pred = reg_model(descriptor_scaled, training=False)\n",
    "        sigma = tf.constant(1e-4, dtype=tf.float32)\n",
    "        loss = tf.reduce_mean(tf.square((JV_exp - jv_pred) / sigma))\n",
    "        log_likelihood = -0.5 * loss\n",
    "#        tf.print(\"ðŸ“ˆ log_likelihood:\", log_likelihood)\n",
    "\n",
    "        return tf.clip_by_value(log_likelihood, -1e6, 0.0)\n",
    "\n",
    "    except Exception as e:\n",
    "        tf.print(\"âš ï¸ Exception in log_prob():\", e)\n",
    "        return tf.constant(-1e6, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# ------------- Sampling Setup -------------\n",
    "\n",
    "num_results = 100\n",
    "num_burnin_steps = 50\n",
    "num_params = 15\n",
    "\n",
    "filename = 'mcmc_samples.dat'\n",
    "if os.path.exists(filename):\n",
    "    os.remove(filename)\n",
    "\n",
    "mmap_samples = np.memmap(filename, dtype='float32', mode='w+', shape=(num_results, num_params))\n",
    "\n",
    "initial_theta = 0.0 + 0.0001 * np.random.randn(num_params)\n",
    "initial_state = [tf.constant(v, dtype=tf.float32) for v in initial_theta]\n",
    "\n",
    "nuts_kernel = tfp.mcmc.NoUTurnSampler(\n",
    "    target_log_prob_fn=log_prob,\n",
    "    step_size=0.0001\n",
    ")\n",
    "\n",
    "adaptive_kernel = tfp.mcmc.DualAveragingStepSizeAdaptation(\n",
    "    inner_kernel=nuts_kernel,\n",
    "    num_adaptation_steps=int(0.8 * num_burnin_steps),\n",
    "    target_accept_prob=0.8\n",
    ")\n",
    "\n",
    "# ------------- Run Sampling -------------\n",
    "def run_nuts():\n",
    "    t0 = time.time()\n",
    "\n",
    "    samples, log_probs = tfp.mcmc.sample_chain(\n",
    "        num_results=num_results,\n",
    "        num_burnin_steps=num_burnin_steps,\n",
    "        current_state=initial_state,\n",
    "        kernel=adaptive_kernel,\n",
    "        trace_fn=lambda cs, kr: kr.inner_results.target_log_prob\n",
    "    )\n",
    "\n",
    "    samples_np = tf.stack(samples, axis=1).numpy()  # shape: (num_results, 15)\n",
    "    mmap_samples[:] = samples_np\n",
    "    mmap_samples.flush()\n",
    "\n",
    "#    print(f\"âœ… Sampling complete. Time: {time.time() - t0:.2f} seconds\")\n",
    " #   print(\"ðŸ“ Samples saved to:\", filename)\n",
    "  #  print(\"ðŸ“‰ Mean log_prob:\", tf.reduce_mean(log_probs).numpy())\n",
    "    return samples_np, log_probs\n",
    "\n",
    "samples, log_probs = run_nuts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d7e0c-8582-406f-b9a8-bef2e372ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_last = samples_2[-1, :]  # Shape: (15,)\n",
    "sim_JVs, _ = check_plot(theta_last, x_vals, 1)  # x is your experimental temperature grid\n",
    "\n",
    "# ---- Plot simulated vs experimental JV curves ----\n",
    "\n",
    "fig, ax = plt.subplots(5, 1, figsize=(8, 10))\n",
    "for i in range(5):\n",
    "    ax[i].plot(sim_JVs[i, :], '--', label='Simulated')\n",
    "    ax[i].plot(JV_exp[i, :], label='Experimental')\n",
    "    ax[i].set_ylabel(f'Curve {i+1}')\n",
    "    ax[i].legend()\n",
    "ax[-1].set_xlabel(\"Voltage Index (or Time)\")\n",
    "plt.suptitle(\"Simulated vs Experimental JV Curves\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- Extract material properties over a finer x grid ----\n",
    "\n",
    "x_step = np.linspace(min(x), max(x), 50)\n",
    "\n",
    "par_in = []\n",
    "for i in range(samples_2.shape[0]):\n",
    "    try:\n",
    "        _, par_input = check_plot(samples_2[i, :], x_vals, 0)\n",
    "        if par_input.shape == (len(x_vals), 5):\n",
    "            par_in.append(par_input)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping sample {i}: {e}\")\n",
    "\n",
    "par_in = np.array(par_in)  # Shape: (n_samples, 50, 5)\n",
    "print(par_in.shape)\n",
    "\n",
    "# ---- Discard burn-in (optional) ----\n",
    "if par_in.shape[0] > 2000:\n",
    "    par_in = par_in[2000:, :, :]\n",
    "\n",
    "# ---- Convert from log-space to actual values ----\n",
    "par_in = np.exp(par_in)  # Final shape: (n_good_samples, 50, 5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357d6b41-5db6-4ce7-bdf3-0a7784516744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uncertain(x,y):\n",
    "    \n",
    "    mu = np.mean(y,axis = 0)\n",
    "    std = np.std(y, axis = 0)\n",
    "    plt.fill_between(x, mu+std,mu-std,alpha=0.1,color='grey')\n",
    "    plt.plot(x,mu,color='black')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [8, 10]\n",
    "plt.rcParams.update({'font.size': 16})\n",
    " \n",
    "fig = plt.figure()\n",
    "y_label = ['Conc.[cm-3]','Conc.[cm-3]', r'$\\tau$ [s]', 'SRV [cm/S]','SRV [cm/S]']\n",
    "x_labels = ['-1/530' ,'-1/580','-1/630','-1/680']\n",
    "title = ['Zn emitter doping' , 'Si base doping' ,'bulk lifetime','Front SRV', 'Rear SRV']\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    plt.subplot(5,1,i+1)\n",
    "    \n",
    "    l1=plot_uncertain(x_vals,par_in[:,:,i]) \n",
    "   \n",
    "    plt.yscale('log') \n",
    "    plt.ylabel(y_label[i])\n",
    "    plt.xticks([-1000/530,-1000/580,-1000/630,-1000/680],[])\n",
    "    plt.title(title[i],fontsize=15,fontweight='bold')\n",
    "    plt.xlim(-1000/530,-1000/680)\n",
    "    \n",
    "  \n",
    "plt.xticks([-1000/530,-1000/580,-1000/630,-1000/680], x_labels)\n",
    "\n",
    "plt.xlabel(r'-1/T [1/C]') \n",
    "\n",
    "fig.align_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add41da-0b0c-447f-8af7-c1d4f83c3b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_map = np.argmax(log_probs)\n",
    "theta_last = samples[idx_map, :]  # Shape: (15,)\n",
    "sim_JVs, _ = check_plot(theta_last, x_vals, 1)  # x is your experimental temperature grid\n",
    "\n",
    "# ---- Plot simulated vs experimental JV curves ----\n",
    "\n",
    "fig, ax = plt.subplots(5, 1, figsize=(8, 10))\n",
    "for i in range(5):\n",
    "    ax[i].plot(sim_JVs[i, :], '--', label='Simulated')\n",
    "    ax[i].plot(JV_exp[i, :], label='Experimental')\n",
    "    ax[i].set_ylabel(f'Curve {i+1}')\n",
    "    ax[i].legend()\n",
    "ax[-1].set_xlabel(\"Voltage Index (or Time)\")\n",
    "plt.suptitle(\"Simulated vs Experimental JV Curves\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a775a5f4-b249-4a9c-852f-e9720c011fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, par_input = check_plot(samples[idx_map,:], x_vals, 0)\n",
    "par_in = np.exp(par_input)\n",
    "\n",
    "def plot_uncertain_2(x,y):\n",
    "    plt.plot(x,y,color='black')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [8, 10]\n",
    "plt.rcParams.update({'font.size': 16})\n",
    " \n",
    "fig = plt.figure()\n",
    "y_label = ['Conc.[cm-3]','Conc.[cm-3]', r'$\\tau$ [s]', 'SRV [cm/S]','SRV [cm/S]']\n",
    "x_labels = ['-1/530' ,'-1/580','-1/630','-1/680']\n",
    "title = ['Zn emitter doping' , 'Si base doping' ,'bulk lifetime','Front SRV', 'Rear SRV']\n",
    "\n",
    "for i in range(5):\n",
    "    plt.subplot(5,1,i+1)\n",
    "    \n",
    "    l1=plot_uncertain_2(x_vals,par_in[:,i]) \n",
    "    plt.yscale('log') \n",
    "    plt.ylabel(y_label[i])\n",
    "    plt.xticks([-1000/530,-1000/580,-1000/630,-1000/680],[])\n",
    "    plt.title(title[i],fontsize=15,fontweight='bold')\n",
    "    plt.xlim(-1000/530,-1000/680)\n",
    "    \n",
    "  \n",
    "plt.xticks([-1000/530,-1000/580,-1000/630,-1000/680], x_labels)\n",
    "\n",
    "plt.xlabel(r'-1/T [1/C]') \n",
    "\n",
    "fig.align_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd666f8-6801-4c3f-9404-fbd5a1c61d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
