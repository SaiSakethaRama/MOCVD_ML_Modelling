{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad980693-5465-4cd9-84ed-c8c008a09b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 14:29:50.901785: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-28 14:29:50.924760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753712990.946699   22331 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753712990.953240   22331 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753712990.976022   22331 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753712990.976047   22331 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753712990.976049   22331 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753712990.976050   22331 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-28 14:29:50.982149: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, Lambda,Conv1D,Conv2DTranspose, LeakyReLU,Activation,Flatten,Reshape\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import load_model\n",
    "import emcee\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe18027-eb79-4615-abd6-5f2f44f22fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "JV_raw = np.loadtxt(\"/mnt/c/Users/pssrg/Downloads/GaAs_sim_nJV.txt\")\n",
    "par = np.loadtxt(\"/mnt/c/Users/pssrg/Downloads/GaAs_sim_label.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04453f61-2c26-4942-af6f-768df2cd6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv1DTranspose(input_tensor, filters, kernel_size, strides ):\n",
    "    \n",
    "    x = Lambda(lambda x: K.expand_dims(x, axis=2))(input_tensor)\n",
    "    x = Conv2DTranspose(filters=filters, kernel_size=(kernel_size, 1), strides=(strides, 1),padding='SAME')(x)\n",
    "    x = Lambda(lambda x: K.squeeze(x, axis=2))(x)\n",
    "    return x\n",
    "\n",
    "#Covert labels from log10 form to log\n",
    "        \n",
    "def log10_ln(x):\n",
    "    return np.log(np.power(10,x))\n",
    "\n",
    "par = log10_ln(par)\n",
    "\n",
    "\n",
    "#Data normalization for the whole JV dataset\n",
    "\n",
    "def min_max(x):\n",
    "    min = np.min(x)\n",
    "    max = np.max(x)\n",
    "    return (x-min)/(max-min),max,min\n",
    "\n",
    "#Normalize raw JV data\n",
    "\n",
    "JV_norm,JV_max,JV_min = min_max(JV_raw)\n",
    "\n",
    "#Normalize JV descriptors column-wise\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "par_n = scaler.fit_transform(par)   \n",
    "\n",
    "#create training and testing datset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(JV_norm,par_n, test_size=0.2)\n",
    "\n",
    "#add in Gaussian noise to train the denoising Autoencoder\n",
    "\n",
    "X_train_nos = X_train+0.002 * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape) \n",
    "\n",
    "X_test_nos = X_test+0.002 * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6569f28-8504-4401-84f7-a8bac6765988",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "label_dim = y_train.shape[1]\n",
    "\n",
    "max_filter = 256\n",
    "strides = [5, 2, 2]\n",
    "kernel = [7, 5, 3]\n",
    "batch_size = 128\n",
    "x = Input(shape=(input_dim,))  # JVi (noisy input)\n",
    "map_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0425787-bb36-4061-9e5b-43047039954d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1753713441.283046   22581 service.cc:152] XLA service 0x7f6e0400bd20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1753713441.283126   22581 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-07-28 14:37:21.344695: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1753713441.634063   22581 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-07-28 14:37:24.137609: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197', 268 bytes spill stores, 268 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:24.192515: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_473', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:24.217613: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197', 348 bytes spill stores, 348 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:24.220983: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197', 468 bytes spill stores, 468 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:24.458950: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197_0', 128 bytes spill stores, 128 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:24.470009: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:24.521407: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_473', 76 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:24.521530: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197', 380 bytes spill stores, 380 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:24.582925: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_473', 60 bytes spill stores, 60 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:24.789105: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_473', 268 bytes spill stores, 268 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:24.789334: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197', 392 bytes spill stores, 392 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:24.987519: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_473_0', 208 bytes spill stores, 208 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 15/125\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0633"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753713448.741800   22581 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m120/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0258"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 14:37:33.877927: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:33.999237: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197', 464 bytes spill stores, 464 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:34.131014: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197', 348 bytes spill stores, 348 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:34.131046: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197', 240 bytes spill stores, 240 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:34.383844: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197_0', 108 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:34.429935: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_473', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:34.508448: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:34.609247: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197', 368 bytes spill stores, 368 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:34.801001: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_473', 492 bytes spill stores, 492 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:34.888560: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_473', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:34.991516: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_473', 820 bytes spill stores, 820 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:34.995342: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_473', 456 bytes spill stores, 456 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:35.044808: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_473', 192 bytes spill stores, 192 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:35.086878: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_473', 584 bytes spill stores, 584 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:35.147958: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_197', 416 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:35.276322: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_473', 416 bytes spill stores, 416 bytes spill loads\n",
      "\n",
      "2025-07-28 14:37:35.491268: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_473_0', 208 bytes spill stores, 208 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 74ms/step - loss: 0.0250\n",
      "Epoch 2/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 7.4749e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 3.2153e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.9136e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3718e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 9.8697e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 7.3650e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 6.0629e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 6.0391e-05\n",
      "Epoch 10/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 4.5908e-05\n",
      "Epoch 11/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.8087e-05\n",
      "Epoch 12/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.4332e-05\n",
      "Epoch 13/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.2316e-05\n",
      "Epoch 14/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.9339e-05\n",
      "Epoch 15/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.7540e-05\n",
      "Epoch 16/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.2544e-05\n",
      "Epoch 17/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.0927e-05\n",
      "Epoch 18/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 2.0478e-05\n",
      "Epoch 19/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2.1443e-05\n",
      "Epoch 20/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.7269e-05\n",
      "Epoch 21/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.6367e-05\n",
      "Epoch 22/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.4803e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.5442e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.4128e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.4304e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6270e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2399e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3812e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 1.2373e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0416e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1892e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.3012e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.1174e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0452e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0795e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 1.0084e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 8.7227e-06\n",
      "Epoch 38/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.0928e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.2154e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 9.9393e-06\n",
      "Epoch 41/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 7.3896e-06\n",
      "Epoch 42/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 7.8504e-06\n",
      "Epoch 43/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 7.8110e-06\n",
      "Epoch 44/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 1.6168e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 6.9404e-06\n",
      "Epoch 46/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 6.5172e-06\n",
      "Epoch 47/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 8.5630e-06\n",
      "Epoch 48/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 7.7908e-06\n",
      "Epoch 49/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 9.9772e-06\n",
      "Epoch 50/50\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1.0762e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f6fff33d9d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input: material descriptors (shape = [None, 5])\n",
    "z_in = Input(shape=(label_dim,))  # label_dim = 5\n",
    "\n",
    "# Expand through Dense layers\n",
    "z1 = Dense(100, activation='relu')(z_in)\n",
    "z1 = Dense(max_filter * map_size, activation='relu')(z1)\n",
    "z1 = Reshape((map_size, 1, max_filter))(z1)\n",
    "\n",
    "# Upsample through deconvolution layers (like decoder)\n",
    "z2 = Conv2DTranspose(max_filter // 2, (kernel[2], 1), strides=(strides[2], 1), padding='same')(z1)\n",
    "z2 = Activation('relu')(z2)\n",
    "\n",
    "z3 = Conv2DTranspose(max_filter // 4, (kernel[1], 1), strides=(strides[1], 1), padding='same')(z2)\n",
    "z3 = Activation('relu')(z3)\n",
    "\n",
    "z4 = Conv2DTranspose(1, (kernel[0], 1), strides=(strides[0], 1), padding='same')(z3)\n",
    "decoded_x = Activation('sigmoid')(z4)\n",
    "\n",
    "# Squeeze dimensions safely\n",
    "decoded_x = Lambda(lambda x: K.squeeze(x, axis=2), output_shape=lambda s: (s[0], s[1], s[3]))(decoded_x)\n",
    "decoded_x = Lambda(lambda x: K.squeeze(x, axis=2), output_shape=lambda s: (s[0], s[1]))(decoded_x)\n",
    "\n",
    "# Final model\n",
    "reg = Model(z_in, decoded_x)\n",
    "\n",
    "# Compile and train\n",
    "reg.compile(loss='mse', optimizer='adam')\n",
    "reg.fit(y_train, X_train, shuffle=True, batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a81fadc-183e-4c10-a774-90656c7e008b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 14:38:19.720062: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m479/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 14:38:22.956557: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_27', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "train mse: 0.000005\n",
      "test mse: 0.000006\n"
     ]
    }
   ],
   "source": [
    "y_hat_train = reg.predict(y_train)\n",
    "y_hat_test = reg.predict(y_test)\n",
    "\n",
    "\n",
    "#voltage sweep\n",
    "v_sweep = np.linspace (0,1.1,100)\n",
    "\n",
    "v_total =np.tile(v_sweep,5).reshape(1,-1)\n",
    "\n",
    "\n",
    "mse = mean_squared_error\n",
    "mse_train = mse(y_hat_train,X_train)\n",
    "mse_test = mse(y_hat_test,X_test)\n",
    "\n",
    "print ('train mse: %.6f' % (mse_train))\n",
    "print ('test mse: %.6f' % (mse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39da4944-486e-4355-9500-ca70673c8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reg.save('./TrainedModel/GaAs_reg.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
